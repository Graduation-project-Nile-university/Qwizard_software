{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-05T16:56:31.691249Z","iopub.status.busy":"2024-02-05T16:56:31.690906Z","iopub.status.idle":"2024-02-05T16:56:31.695586Z","shell.execute_reply":"2024-02-05T16:56:31.694658Z","shell.execute_reply.started":"2024-02-05T16:56:31.691223Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import json\n","import csv\n","\n","with open('dev-v2.0.json') as json_file:\n","\tdata = json.load(json_file)\n","\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_texts_queries_answers(path):\n","    with open(path, 'rb') as f:\n","        squad_dict = json.load(f)\n","    texts, queries, answers = [], [], []\n","    for group in squad_dict['data']:\n","        for passage in group['paragraphs']:\n","            context = passage['context']\n","            for qa in passage['qas']:\n","                question = qa['question']\n","                for answer in qa['answers']:\n","                    texts.append(context)\n","                    queries.append(question)\n","                    answers.append(answer)\n","    return texts, queries, answers\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["texts, queries, answers = generate_texts_queries_answers('dev-v2.0.json')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["queries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["answers_text = [item['text'] for item in answers]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["answers_text"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["datadf = pd.DataFrame({'context': texts, 'Questions': queries, 'answers': answers_text})\n","print(datadf)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["datadf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["datadf = datadf.drop_duplicates()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","datadf.to_csv('sample.csv',index = False)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Loading dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-05T16:29:28.553697Z","iopub.status.busy":"2024-02-05T16:29:28.552673Z","iopub.status.idle":"2024-02-05T16:29:28.740195Z","shell.execute_reply":"2024-02-05T16:29:28.739236Z","shell.execute_reply.started":"2024-02-05T16:29:28.553662Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv(\"/kaggle/input/graduation/sample.csv\")\n","data"]},{"cell_type":"markdown","metadata":{},"source":["# Question Generation not good in generation because it needs answer as input\n","hugging face : https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T21:05:15.106847Z","iopub.status.busy":"2024-02-02T21:05:15.106555Z","iopub.status.idle":"2024-02-02T21:05:29.525694Z","shell.execute_reply":"2024-02-02T21:05:29.524838Z","shell.execute_reply.started":"2024-02-02T21:05:15.106820Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelWithLMHead, AutoTokenizer\n","\n","tokenizer_t5base = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n","model_t5base = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T21:11:35.286774Z","iopub.status.busy":"2024-02-02T21:11:35.286354Z","iopub.status.idle":"2024-02-02T21:11:35.894742Z","shell.execute_reply":"2024-02-02T21:11:35.893746Z","shell.execute_reply.started":"2024-02-02T21:11:35.286739Z"},"trusted":true},"outputs":[],"source":["def get_question(answer, context, max_length=64):\n","    input_text = \"answer: %s  context: %s </s>\" % (answer, context)\n","    features = tokenizer_t5base([input_text], return_tensors='pt')\n","    output = model_t5base.generate(input_ids=features['input_ids'], \n","               attention_mask=features['attention_mask'],\n","               max_length=max_length)\n","\n","    return tokenizer_t5base.decode(output[0])\n","\n","context = \"Manuel has created RuPERTa-base with the support of HF-Transformers and Google\"\n","answer = \"Manuel\"\n","\n","get_question(answer, context)\n"]},{"cell_type":"markdown","metadata":{},"source":["## T5"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-05T16:34:16.007893Z","iopub.status.busy":"2024-02-05T16:34:16.007527Z","iopub.status.idle":"2024-02-05T16:35:23.278540Z","shell.execute_reply":"2024-02-05T16:35:23.274475Z","shell.execute_reply.started":"2024-02-05T16:34:16.007864Z"},"trusted":true},"outputs":[],"source":["from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","tokenizer = T5Tokenizer.from_pretrained(\"t5-3b\")\n","model = T5ForConditionalGeneration.from_pretrained(\"t5-3b\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-05T16:35:31.844337Z","iopub.status.busy":"2024-02-05T16:35:31.843597Z","iopub.status.idle":"2024-02-05T16:36:56.566297Z","shell.execute_reply":"2024-02-05T16:36:56.565406Z","shell.execute_reply.started":"2024-02-05T16:35:31.844299Z"},"trusted":true},"outputs":[],"source":["t5_questions_results = []\n","def get_question_t5(context):\n","    text = \"context: %s. Generate Question  \"%context\n","    input_ids = tokenizer(text, return_tensors=\"pt\")\n","    generate = model.generate(**input_ids)\n","    t5_questions_results.append(tokenizer.decode(generate[0]))\n","data[\"context\"].sample(n=10).apply(lambda x : get_question_t5(x))\n","t5_questions_results"]},{"cell_type":"markdown","metadata":{},"source":["## Bert\n","\n","hugging_face link: https://huggingface.co/voidful/context-only-question-generator"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-05T16:54:26.459005Z","iopub.status.busy":"2024-02-05T16:54:26.457987Z","iopub.status.idle":"2024-02-05T16:54:49.599215Z","shell.execute_reply":"2024-02-05T16:54:49.598162Z","shell.execute_reply.started":"2024-02-05T16:54:26.458969Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"voidful/context-only-question-generator\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"voidful/context-only-question-generator\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-05T17:41:55.446512Z","iopub.status.busy":"2024-02-05T17:41:55.445833Z","iopub.status.idle":"2024-02-05T17:41:55.939994Z","shell.execute_reply":"2024-02-05T17:41:55.939023Z","shell.execute_reply.started":"2024-02-05T17:41:55.446476Z"},"trusted":true},"outputs":[],"source":["def generate_question_using_context_only(context):\n","    inputs = tokenizer(context, return_tensors=\"pt\")\n","    outputs = model.generate(inputs[\"input_ids\"])\n","    return tokenizer.decode(outputs[0]) \n","\n","context = \"byden is the american president\"\n","generate_question_using_context_only(context)"]},{"cell_type":"markdown","metadata":{},"source":["# t5 trained on squad\n","model on hugging face:\n","https://huggingface.co/p208p2002/bart-squad-qg-hl?text=Harry+Potter+is+a+series+of+seven+fantasy+novels+written+by+British+author%2C+%5BHL%5DJ.+K.+Rowling%5BHL%5D.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","  \n","tokenizer_t5squad = AutoTokenizer.from_pretrained(\"p208p2002/t5-squad-qg-hl\", use_fast=False)\n","\n","model_t5squad = AutoModelForSeq2SeqLM.from_pretrained(\"p208p2002/t5-squad-qg-hl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","import pandas as pd\n","\n","# Load T5 model and tokenizer\n","tokenizer_t5squad = AutoTokenizer.from_pretrained(\"p208p2002/t5-squad-qg-hl\", use_fast=False)\n","model_t5squad = AutoModelForSeq2SeqLM.from_pretrained(\"p208p2002/t5-squad-qg-hl\")\n","\n","# Initialize an empty list to store generated outputs\n","generated_outputs = []\n","\n","# Loop through each context in the dataframe\n","for input_text in datadf[\"context\"]:\n","    input_ids = tokenizer_t5squad.encode(input_text, return_tensors=\"pt\")\n","    output = model_t5squad.generate(input_ids, max_length=64)\n","    generated_text = tokenizer_t5squad.batch_decode(output)[0]\n","    generated_outputs.append(generated_text)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["\n","\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","import pandas as pd\n","\n","# Load T5 model and tokenizer\n","tokenizer_t5squad = AutoTokenizer.from_pretrained(\"p208p2002/t5-squad-qg-hl\", use_fast=False)\n","model_t5squad = AutoModelForSeq2SeqLM.from_pretrained(\"p208p2002/t5-squad-qg-hl\")\n","\n","# Initialize an empty list to store generated outputs\n","generated_outputs = []\n","\n","# Loop through each context in the dataframe\n","for input_text in datadf[\"context\"]:\n","    input_ids = tokenizer_t5squad.encode(input_text, return_tensors=\"pt\")\n","    output = model_t5squad.generate(input_ids, max_length=64)\n","    generated_text = tokenizer_t5squad.batch_decode(output)[0]\n","    generated_outputs.append(generated_text)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install sentencepiece\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","# Load T5 model and tokenizer\n","tokenizer_t5squad = AutoTokenizer.from_pretrained(\"p208p2002/t5-squad-qg-hl\", use_fast=False)\n","model_t5squad = AutoModelForSeq2SeqLM.from_pretrained(\"p208p2002/t5-squad-qg-hl\")\n","\n","# List to store generated outputs\n","generated_outputs_t5 = []\n","\n","# Loop through the DataFrame and generate questions using T5\n","for context_text in datadf[\"context\"]:\n","    # Tokenize and encode the input text\n","    input_ids = tokenizer_t5squad.encode(context_text, return_tensors=\"pt\", max_length=512, truncation=True)\n","\n","    # Generate questions using T5\n","    output_ids = model_t5squad.generate(input_ids, max_length=64)\n","    generated_text = tokenizer_t5squad.batch_decode(output_ids, skip_special_tokens=True)[0]\n","    \n","    # Append generated text to the list\n","    generated_outputs_t5.append(generated_text)\n","\n","# Display the generated outputs\n","for context, generated_text in zip(datadf[\"context\"], generated_outputs_t5):\n","    print(f\"Context: {context}\")\n","    print(f\"Generated Question: {generated_text}\\n\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# bert for question answering\n","hugging face link: https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertForQuestionAnswering"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-05T17:19:17.018569Z","iopub.status.busy":"2024-02-05T17:19:17.017528Z","iopub.status.idle":"2024-02-05T17:19:20.753532Z","shell.execute_reply":"2024-02-05T17:19:20.752740Z","shell.execute_reply.started":"2024-02-05T17:19:17.018533Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, BertForQuestionAnswering\n","import torch\n","tokenizer_bert = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","model_bert = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-05T17:33:54.390528Z","iopub.status.busy":"2024-02-05T17:33:54.389796Z","iopub.status.idle":"2024-02-05T17:33:54.441274Z","shell.execute_reply":"2024-02-05T17:33:54.440111Z","shell.execute_reply.started":"2024-02-05T17:33:54.390495Z"},"trusted":true},"outputs":[],"source":["for _, row in datadf.iterrows():\n","    question = row['Questions']\n","    text = row['context']\n","\n","    inputs = tokenizer_bert(question, text, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = model_bert(**inputs)\n","\n","    answer_start_index = outputs.start_logits.argmax()\n","    answer_end_index = outputs.end_logits.argmax()\n","\n","    predict_answer_tokens = inputs.input_ids[0, answer_start_index: answer_end_index + 1]\n","    predicted_answer = tokenizer_bert.decode(predict_answer_tokens, skip_special_tokens=True)\n","\n","    answers.append(predicted_answer)"]},{"cell_type":"markdown","metadata":{},"source":["# PREDICTION\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer, BertForQuestionAnswering\n","import torch\n","\n","tokenizer_bert = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","model_bert = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","\n","# Load the sample data\n","datadf_sample = pd.read_csv('sample.csv')\n","\n","# Get the last two rows\n","last_two_rows = datadf_sample.sample(n=2)\n","\n","for _, row in last_two_rows.iterrows():\n","    question = row['Questions']\n","    text = row['context']\n","\n","    inputs = tokenizer_bert(question, text, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = model_bert(**inputs)\n","\n","    answer_start_index = outputs.start_logits.argmax()\n","    answer_end_index = outputs.end_logits.argmax()\n","\n","    predict_answer_tokens = inputs.input_ids[0, answer_start_index: answer_end_index + 1]\n","    predicted_answer = tokenizer_bert.decode(predict_answer_tokens, skip_special_tokens=True)\n","\n","    print(predicted_answer)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["from transformers import AutoTokenizer, BertForQuestionAnswering\n","import torch\n","import pandas as pd\n","\n","tokenizer_bert = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","model_bert = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","\n","# Load the sample data\n","datadf_sample = pd.read_csv('sample.csv')\n","\n","# Get the last two rows\n","last_two_rows = datadf_sample.sample(n=2)\n","\n","for _, row in last_two_rows.iterrows():\n","    question = row['Questions']\n","    text = row['context']\n","\n","    inputs = tokenizer_bert(question, text, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = model_bert(**inputs)\n","\n","    answer_start_index = outputs.start_logits.argmax()\n","    answer_end_index = outputs.end_logits.argmax()\n","\n","    predict_answer_tokens = inputs.input_ids[0, answer_start_index: answer_end_index + 1]\n","    predicted_answer = tokenizer_bert.decode(predict_answer_tokens, skip_special_tokens=True)\n","\n","    # Print question and predicted answer\n","    print(f\"Question: {question}\")\n","    print(f\"Predicted Answer: {predicted_answer}\")\n","    print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer, BertForQuestionAnswering\n","import torch\n","import pandas as pd\n","\n","tokenizer_bert = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","model_bert = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","\n","# Load the sample data\n","datadf_sample = pd.read_csv('sample.csv')\n","\n","# Get the last two rows\n","last_two_rows = datadf_sample.sample(n=2)\n","\n","for _, row in last_two_rows.iterrows():\n","    question = row['Questions']\n","    text = row['context']\n","\n","    inputs = tokenizer_bert(question, text, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = model_bert(**inputs)\n","\n","    answer_start_index = outputs.start_logits.argmax()\n","    answer_end_index = outputs.end_logits.argmax()\n","\n","    predict_answer_tokens = inputs.input_ids[0, answer_start_index: answer_end_index + 1]\n","    predicted_answer = tokenizer_bert.decode(predict_answer_tokens, skip_special_tokens=True)\n","\n","    # Print question and predicted answer\n","    print(f\"Question: {question}\")\n","    print(f\"Predicted Answer: {predicted_answer}\")\n","    print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer, BertForQuestionAnswering\n","import torch\n","import pandas as pd\n","\n","tokenizer_bert = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","model_bert = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","\n","# Load the sample data\n","datadf_sample = pd.read_csv('sample.csv')\n","\n","# Get the last two rows\n","last_two_rows = datadf_sample.sample(n=2)\n","\n","for _, row in last_two_rows.iterrows():\n","    question = row['Questions']\n","    text = row['context']\n","\n","    inputs = tokenizer_bert(question, text, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = model_bert(**inputs)\n","\n","    answer_start_index = outputs.start_logits.argmax()\n","    answer_end_index = outputs.end_logits.argmax()\n","\n","    predict_answer_tokens = inputs.input_ids[0, answer_start_index: answer_end_index + 1]\n","    predicted_answer = tokenizer_bert.decode(predict_answer_tokens, skip_special_tokens=True)\n","\n","    # Print question and predicted answer\n","    print(f\"Question: {question}\")\n","    print(f\"Predicted Answer: {predicted_answer}\")\n","    print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer, BertForQuestionAnswering\n","import torch\n","import pandas as pd\n","\n","tokenizer_bert = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","model_bert = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","\n","# Load the sample data\n","datadf_sample = pd.read_csv('sample.csv')\n","\n","# Get the last two rows\n","last_two_rows = datadf_sample.sample(n=2)\n","\n","for _, row in last_two_rows.iterrows():\n","    question = row['Questions']\n","    text = row['context']\n","\n","    inputs = tokenizer_bert(question, text, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = model_bert(**inputs)\n","\n","    answer_start_index = outputs.start_logits.argmax()\n","    answer_end_index = outputs.end_logits.argmax()\n","\n","    predict_answer_tokens = inputs.input_ids[0, answer_start_index: answer_end_index + 1]\n","    predicted_answer = tokenizer_bert.decode(predict_answer_tokens, skip_special_tokens=True)\n","\n","    # Print question and predicted answer\n","    print(f\"Question: {question}\")\n","    print(f\"Predicted Answer: {predicted_answer}\")\n","    print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer, BertForQuestionAnswering\n","import torch\n","import pandas as pd\n","\n","tokenizer_bert = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","model_bert = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","\n","# Load the sample data\n","datadf_sample = pd.read_csv('sample.csv')\n","\n","# Get the last two rows\n","last_two_rows = datadf_sample.sample(n=2)\n","\n","for _, row in last_two_rows.iterrows():\n","    question = row['Questions']\n","    text = row['context']\n","\n","    inputs = tokenizer_bert(question, text, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = model_bert(**inputs)\n","\n","    answer_start_index = outputs.start_logits.argmax()\n","    answer_end_index = outputs.end_logits.argmax()\n","\n","    predict_answer_tokens = inputs.input_ids[0, answer_start_index: answer_end_index + 1]\n","    predicted_answer = tokenizer_bert.decode(predict_answer_tokens, skip_special_tokens=True)\n","\n","    # Print question and predicted answer\n","    print(f\"Question: {question}\")\n","    print(f\"Predicted Answer: {predicted_answer}\")\n","    print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["from transformers import AutoTokenizer, BertForQuestionAnswering\n","import torch\n","import pandas as pd\n","\n","tokenizer_bert = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","model_bert = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","\n","# Load the sample data\n","datadf_sample = pd.read_csv('sample.csv')\n","\n","# Get the last two rows\n","last_two_rows = datadf_sample.sample(n=2)\n","\n","for _, row in last_two_rows.iterrows():\n","    question = row['Questions']\n","    text = row['context']\n","\n","    inputs = tokenizer_bert(question, text, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = model_bert(**inputs)\n","\n","    answer_start_index = outputs.start_logits.argmax()\n","    answer_end_index = outputs.end_logits.argmax()\n","\n","    predict_answer_tokens = inputs.input_ids[0, answer_start_index: answer_end_index + 1]\n","    predicted_answer = tokenizer_bert.decode(predict_answer_tokens, skip_special_tokens=True)\n","\n","    # Print question and predicted answer\n","    print(f\"Question: {question}\")\n","    print(f\"Predicted Answer: {predicted_answer}\")\n","    print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer, BertForQuestionAnswering\n","import torch\n","import pandas as pd\n","\n","tokenizer_bert = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","model_bert = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","\n","# Load the sample data\n","datadf_sample = pd.read_csv('sample.csv')\n","\n","# Get the last two rows\n","last_two_rows = datadf_sample.sample(n=2)\n","\n","for _, row in last_two_rows.iterrows():\n","    question = row['Questions']\n","    text = row['context']\n","\n","    inputs = tokenizer_bert(question, text, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = model_bert(**inputs)\n","\n","    answer_start_index = outputs.start_logits.argmax()\n","    answer_end_index = outputs.end_logits.argmax()\n","\n","    predict_answer_tokens = inputs.input_ids[0, answer_start_index: answer_end_index + 1]\n","    predicted_answer = tokenizer_bert.decode(predict_answer_tokens, skip_special_tokens=True)\n","\n","    # Print question and predicted answer\n","    print(f\"Question: {question}\")\n","    print(f\"Predicted Answer: {predicted_answer}\")\n","    print()\n"]},{"cell_type":"markdown","metadata":{},"source":["# *test one*"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"source":["!pip install nltk\n","!pip install rouge\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk.translate.bleu_score import sentence_bleu\n","from transformers import AutoTokenizer, BertForQuestionAnswering\n","import torch\n","\n","tokenizer_bert = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","model_bert = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","\n","# Load the sample data\n","datadf_sample = pd.read_csv('sample.csv')\n","\n","# Get the last two rows\n","last_two_rows = datadf_sample.sample(n=2)\n","\n","# List to store the predicted answers\n","answers = []\n","\n","for _, row in last_two_rows.iterrows():\n","    question = row['Questions']\n","    text = row['context']\n","\n","    inputs = tokenizer_bert(question, text, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = model_bert(**inputs)\n","\n","    answer_start_index = outputs.start_logits.argmax()\n","    answer_end_index = outputs.end_logits.argmax()\n","\n","    predict_answer_tokens = inputs.input_ids[0, answer_start_index: answer_end_index + 1]\n","    predicted_answer = tokenizer_bert.decode(predict_answer_tokens, skip_special_tokens=True)\n","\n","    # Append the predicted answer to the list\n","    answers.append(predicted_answer)\n","\n","# Now, let's compute the BLEU score\n","references = [[row['answers']] for _, row in last_two_rows.iterrows()]\n","bleu_scores = [sentence_bleu([ref], pred) for ref, pred in zip(references, answers)]\n","\n","print(bleu_scores)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"source":["from rouge import Rouge\n","from transformers import AutoTokenizer, BertForQuestionAnswering\n","import torch\n","\n","tokenizer_bert = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","model_bert = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","\n","# Load the sample data\n","datadf_sample = pd.read_csv('sample.csv')\n","\n","# Get the last two rows\n","last_two_rows = datadf_sample.sample(n=2)\n","\n","# List to store the predicted answers\n","answers = []\n","\n","# Initialize Rouge scorer\n","rouge_scorer = Rouge()\n","\n","for _, row in last_two_rows.iterrows():\n","    question = row['Questions']\n","    text = row['context']\n","\n","    inputs = tokenizer_bert(question, text, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = model_bert(**inputs)\n","\n","    answer_start_index = outputs.start_logits.argmax()\n","    answer_end_index = outputs.end_logits.argmax()\n","\n","    predict_answer_tokens = inputs.input_ids[0, answer_start_index: answer_end_index + 1]\n","    predicted_answer = tokenizer_bert.decode(predict_answer_tokens, skip_special_tokens=True)\n","\n","    # Append the predicted answer to the list\n","    answers.append(predicted_answer)\n","\n","# Now, let's compute the ROUGE score\n","references = [[row['answers']] for _, row in last_two_rows.iterrows()]\n","rouge_scores = [rouge_scorer.get_scores(pred, ref)[0]['rouge-l']['f'] for pred, ref in zip(answers, references)]\n","\n","print(rouge_scores)\n"]},{"cell_type":"markdown","metadata":{},"source":["# test 2\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["from transformers import AutoTokenizer, BertForQuestionAnswering\n","import torch\n","import pandas as pd\n","from nltk.translate.bleu_score import corpus_bleu\n","\n","# Load a sample of the data\n","sample_size = 100  # You can adjust the sample size as needed\n","sample_data = datadf.sample(n=sample_size, random_state=42)\n","\n","# Initialize BERT model and tokenizer\n","tokenizer_bert = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","model_bert = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","\n","# Lists to store predicted answers\n","predicted_answers = []\n","\n","# Iterate through the sample data\n","for _, row in sample_data.iterrows():\n","    question = row['Questions']\n","    text = row['context']\n","\n","    inputs = tokenizer_bert(question, text, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = model_bert(**inputs)\n","\n","    answer_start_index = outputs.start_logits.argmax()\n","    answer_end_index = outputs.end_logits.argmax()\n","\n","    predict_answer_tokens = inputs.input_ids[0, answer_start_index: answer_end_index + 1]\n","    predicted_answer = tokenizer_bert.decode(predict_answer_tokens, skip_special_tokens=True)\n","\n","    predicted_answers.append(predicted_answer)\n","\n","# Add predicted answers to the sample data\n","sample_data['predicted_answers'] = predicted_answers\n","\n","# Tokenize the reference and predicted texts for BLEU metric\n","reference_texts = sample_data['answers'].apply(lambda x: [x]).tolist()\n","predicted_texts = sample_data['predicted_answers'].tolist()\n","\n","# BLEU metric\n","bleu_score = corpus_bleu(reference_texts, predicted_texts)\n","print(f\"BLEU Score: {bleu_score}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import nltk\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download Open Multilingual Wordnet (omw-1.4) dataset\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer, BertForQuestionAnswering\n","import torch\n","import pandas as pd\n","from nltk.translate.meteor_score import single_meteor_score\n","import nltk\n","# Download WordNet dataset\n","nltk.download('wordnet')\n","# Tokenize the reference and predicted texts for METEOR metric\n","tokenized_reference_texts = [ref.split() for ref in sample_data['answers']]\n","tokenized_predicted_texts = [pred.split() for pred in sample_data['predicted_answers']]\n","\n","# METEOR metric\n","meteor_score = sum(single_meteor_score(ref, pred) for ref, pred in zip(tokenized_reference_texts, tokenized_predicted_texts)) / len(tokenized_predicted_texts)\n","print(f\"METEOR Score: {meteor_score}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install rouge\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["from transformers import AutoTokenizer, BertForQuestionAnswering\n","import torch\n","import pandas as pd\n","from rouge import Rouge\n","\n","# Combine tokenized lists into strings for ROUGE metric\n","reference_texts = [' '.join(ref) for ref in tokenized_reference_texts]\n","predicted_texts = [' '.join(pred) for pred in tokenized_predicted_texts]\n","\n","# Initialize ROUGE scorer\n","rouge = Rouge()\n","\n","# ROUGE metric\n","try:\n","    scores = rouge.get_scores(predicted_texts, reference_texts, avg=True)\n","    rouge_score = scores['rouge-1']['f']\n","    print(f\"ROUGE Score: {rouge_score}\")\n","except ValueError as e:\n","    print(f\"Error calculating ROUGE: {e}\")\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4390587,"sourceId":7540065,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
