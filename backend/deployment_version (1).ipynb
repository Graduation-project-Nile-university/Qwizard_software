{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_huggingface \n",
    "\n",
    "!pip install pyPDF2\n",
    "!pip install langchain\n",
    "!pip install huggingface_hub\n",
    "!pip install huggingface_hub[sentencepiece]\n",
    "\n",
    "!pip install --upgrade --quiet  transformers --quiet\n",
    "\n",
    "!pip install langchain_hub\n",
    "!pip install langchain_hub[sentencepiece]\n",
    "!pip install langchain_community\n",
    "\n",
    "!pip install text_generation\n",
    "\n",
    "!pip install sentence-transformers\n",
    "\n",
    "!pip install chromadb\n",
    "\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import StrOutputParser\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "# from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "# from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "# from trl import SFTTrainer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(DEVICE)\n",
    "\n",
    "!huggingface-cli login --token \"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\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "!pip install langchain_chroma\n",
    "\n",
    "!pip install langchain_core.schema.runnable\n",
    "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, BitsAndBytesConfig\n",
    "import torch\n",
    "import torch\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_chroma.vectorstores import Chroma  # Assuming Chroma is accessible from langchain_chroma\n",
    "import joblib\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "\n",
    "# set up scorer\n",
    "from tigerscore import TIGERScorer\n",
    "scorer = TIGERScorer(model_name=\"TIGER-Lab/TIGERScore-7B\", quantized=True) # 4 bit quantization on GPU\n",
    "\n",
    "# Initialize and setup LangChain with Llama Index agents\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             torch_dtype=torch.bfloat16,  # Specify the compute dtype\n",
    "                                             low_cpu_mem_usage=True,      # Use efficient memory handling\n",
    "                                             device_map=\"auto\",           # Automatically map the model to the available device\n",
    "                                             load_in_4bit=bnb_config.load_in_4bit,  # Enable 4-bit quantization\n",
    "                                             bnb_4bit_use_double_quant=bnb_config.bnb_4bit_use_double_quant,\n",
    "                                             bnb_4bit_quant_type=bnb_config.bnb_4bit_quant_type,\n",
    "                                             bnb_4bit_compute_dtype=bnb_config.bnb_4bit_compute_dtype)\n",
    "\n",
    "text_generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=2128)\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read and extract text from PDF\n",
    "reader = PdfReader('All_lectures.pdf')\n",
    "raw_text = ''\n",
    "for page in reader.pages:\n",
    "  text = page.extract_text()\n",
    "  if text:\n",
    "      raw_text += text\n",
    "\n",
    "# Split text and prepare embeddings\n",
    "text_splitter = CharacterTextSplitter(\n",
    "  separator=\"\\n\",\n",
    "  chunk_size=500,\n",
    "  chunk_overlap=20,\n",
    "  length_function=len,\n",
    ")\n",
    "# Split text and prepare embeddings\n",
    "texts = text_splitter.split_text(raw_text)\n",
    "\n",
    "embeddings_file = \"path_to_embeddings.joblib\"\n",
    "if os.path.exists(embeddings_file):\n",
    "  embeddings = joblib.load(embeddings_file)\n",
    "else:\n",
    "  embeddings = HuggingFaceEmbeddings()\n",
    "  joblib.dump(embeddings, embeddings_file)\n",
    "\n",
    "vectorstore = Chroma.from_texts(texts, embeddings)\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate Exam\n",
    "# Generate Exam\n",
    "def generate_exam(query):\n",
    "    # Create prompt from template\n",
    "      prompt_template = \"\"\"\n",
    "      <s>[INST] <<SYS>>\n",
    "      You are a helpful AI exam generator.\n",
    "      Generate a solved exam based on the context provided. Be concise and just include the exam generated.\n",
    "      <</SYS>>\n",
    "      {context}\n",
    "      Question: {question}\n",
    "      Helpful Answer: [/INST]\n",
    "      \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "      prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "\n",
    "\n",
    "      # Chain construction\n",
    "      chain = (\n",
    "          {\"context\": retriever, \"question\": RunnablePassthrough()}  # Ensure retriever.retrieve is used correctly\n",
    "          | prompt\n",
    "          | llm\n",
    "      )\n",
    "      exam_text = chain.invoke(query)\n",
    "      return exam_text, retriever, raw_text\n",
    "\n",
    "# Evaluate Exam\n",
    "def evaluate_exam(instruction, input_context, hypo_output):\n",
    "    scorer = TIGERScorer(model_name=\"TIGER-Lab/TIGERScore-7B\") # 4 bit quantization on GPU\n",
    "    results = scorer.score([instruction], [hypo_output], [input_context])\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Feedback Loop to Correct Exam\n",
    "def correct_exam(exam_text, raw_text):\n",
    "    # Evaluate initial exam\n",
    "    evaluation_results = evaluate_exam(\"Please evaluate the following exam\", raw_text, exam_text)\n",
    "    errors = [result['errors'] for result in evaluation_results if 'errors' in result]\n",
    "    if not errors:\n",
    "        return exam_text  # No errors found, return original exam\n",
    "\n",
    "    # Generate a correction prompt including errors\n",
    "    correction_query = f\"Correct the following exam which is: {original_exam}. based on these errors: {errors}, and here is the content that the exam was made from {retriever}\"\n",
    "    corrected_exam = generate_exam(correction_query)\n",
    "    return corrected_exam\n",
    "\n",
    "# Example usage\n",
    "original_exam = generate_exam(\"generate me an exam with several question types about auto parellelization \")\n",
    "\n",
    "corrected_exam = correct_exam(original_exam, retriever)\n",
    "\n",
    "\n",
    "def format_exam(exam_text):\n",
    "    # Assuming questions are separated by the word \"Question:\"\n",
    "    questions = exam_text.split(\"Question:\")\n",
    "    formatted_exam = \"\\n\".join(f\"Question {i}: {q.strip()}\" for i, q in enumerate(questions) if q)\n",
    "    return formatted_exam\n",
    "\n",
    "\n",
    "\n",
    "print(\"Original Exam:\\n\", format_exam(original_exam[0]))\n",
    "print(\"\\nCorrected Exam:\\n\", format_exam(corrected_exam[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
